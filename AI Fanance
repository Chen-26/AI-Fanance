import os
import re
import json
from bs4 import BeautifulSoup
import bs4
from requests import RequestException
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from pyquery import PyQuery as pq
import requests
import time as t
import execjs
from selenium import webdriver
import pandas as pd

driver = webdriver.Chrome()
driver.get('https://rili.jin10.com/day/2013-01-01')
wait = WebDriverWait(driver, 10)


# //*[@id="__layout"]/div/div[2]/div/div[2]/div/div[1]/div[2]/ul/li[1]
# /html/body/div[1]/div/div/div[2]/div/div[2]/div/div[1]/div[2]/ul/li[1]
def change_page():
    a = 1
    while (a < 100000):

        pg = driver.find_element_by_xpath("//*[@id='__layout']/div/div[2]/div/div[2]/div/div[1]/div[2]/ul/li[1]")
        pg.click()
        t.sleep(10)
        get_data()



        pg = driver.find_element_by_xpath("//*[@id='__layout']/div/div[2]/div/div[2]/div/div[1]/div[2]/ul/li[2]")
        pg.click()
        t.sleep(10)
        get_data()



        pg = driver.find_element_by_xpath("//*[@id='__layout']/div/div[2]/div/div[2]/div/div[1]/div[2]/ul/li[3]")
        pg.click()
        t.sleep(10)
        get_data()



        pg = driver.find_element_by_xpath("//*[@id='__layout']/div/div[2]/div/div[2]/div/div[1]/div[2]/ul/li[4]")
        pg.click()
        t.sleep(10)
        get_data()



        pg = driver.find_element_by_xpath("//*[@id='__layout']/div/div[2]/div/div[2]/div/div[1]/div[2]/ul/li[5]")
        pg.click()
        t.sleep(10)
        get_data()





        try:
            pg = driver.find_element_by_xpath("//*[@id='__layout']/div/div[2]/div/div[2]/div/div[1]/div[2]/ul/li[6]")
            pg.click()
            t.sleep(10)
            get_data()
        except:
            pass


        try:
            pg = driver.find_element_by_xpath("//*[@id='__layout']/div/div[2]/div/div[2]/div/div[1]/div[2]/ul/li[7]")
            pg.click()
            t.sleep(10)
            get_data()
        except:
            pass


        pg = driver.find_element_by_xpath("//*[@id='__layout']/div/div[2]/div/div[2]/div/div[1]/div[2]/div[2]")
        pg.click()
        t.sleep(3)


        a = a + 1


def get_data():
    driver.switch_to.window(driver.window_handles[-1])
    url = driver.current_url
    rq = requests.get(url)
    rq.encoding = 'UTF-8'
    # soup = BeautifulSoup(rq.text, 'lxml')
    # l = len(soup.find_all('td', class_='jin-table-column'))
    # i = 3
    # rs = []
    result = {
        "time": [],  # 时间
        "data": [],  # 数据
        "previous": [],  # 前值
        "forcast": [],  # 预测值
        "reality": [],  # 实际值
        "influence": []  # 影响
    }
    # while i < l:
    #    rs.append(soup.find_all('td', class_='jin-table-column')[i].text)
    #    i = i + 9
    # for i in rs:
    #   print(i)
    html = bs4.BeautifulSoup(rq.text, "html.parser")
    all_job = html.find("tbody").find_all("tr")
    for date in all_job:
        time = date.find("td", class_=re.compile("jin-table-column")).text
        data = date.find("span", class_=re.compile("data-name-text")).text
        previous = date.find("div", class_=re.compile("cell tooltip")).text
        forcast = date.find_all("td", limit=5)[4]
        reality = date.find("span", class_=re.compile("pubbed")).text
        influence = date.find("div", class_=re.compile("cell affect")).text
        result["time"].append(time)
        result["data"].append(data)
        result["previous"].append(previous)
        result["forcast"].append(forcast.text)
        result["reality"].append(reality)
        result["influence"].append(influence)
    df = pd.DataFrame.from_dict(result, orient="index")
    filename = driver.find_element_by_xpath('//*[@id="__layout"]/div/div[2]/div/div[2]/div/div[1]/div[1]/div[2]/div['
                                            '1]/div[1]/span').text
    os.chdir('E:\爬虫csv 13至今')
    df.to_csv(filename + '.csv', encoding="utf_8_sig")


def star():
    url = driver.current_url
    rq = requests.get(url)
    soup = BeautifulSoup(rq.text, 'lxml')
    l = len(soup.find_all('div i', style={'color': 'rgb(221, 221, 221)' and 'font-size: 12px'}))
    stars = 5 - l
    print(stars)


def main():
    # star()
    change_page()
    driver.close()


if __name__ == '__main__':
    main()
